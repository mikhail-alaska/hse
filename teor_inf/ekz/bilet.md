## 1.Понятие энтропии случайной величины. Основные свойства энтропии (верхняя и нижняя оценки на величину энтропии дискретной СВ)

Понятие энтропии - пусть U - дискретная случайная величина: которая принимает значения
из множества *u*. Тогда энтропия - это ожидаемый сюрприз случайной величины:
$$
H(U)=E(s(u))=\Sigma_{u\in U} p(u) log_2 \frac {1}{p(u}
$$
Энтропия представляет собой ожидаемую величину неожиданности, которую имеет распределение.
Интуитивно понятно, что чем больше ожидаемая "неожиданность" или энтропия распределения, тем
труднее его представить и тем труднее предсказать поведение источника

Свойства энтропии

![2025-06-14-at-19-24-53.png](bilet-img/2025-06-14-at-19-24-53.png)
![2025-06-14-at-19-25-17.png](bilet-img/2025-06-14-at-19-25-17.png)
![2025-06-14-at-19-25-41.png](bilet-img/2025-06-14-at-19-25-41.png)

## 2. Понятие Hq(U) и связь H(U), U~p и Hq(U)

![2025-06-14-at-19-35-07.png](bilet-img/2025-06-14-at-19-35-07.png)
![2025-06-14-at-19-35-29.png](bilet-img/2025-06-14-at-19-35-29.png)

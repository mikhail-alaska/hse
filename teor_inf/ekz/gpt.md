### 1. Понятие энтропии случайной величины. Основные свойства энтропии

Энтропия H(U) – это средняя «неожиданность» (или «сюрприз») при наблюдении исходов случайной величины U.

* Представьте, что вы отгадываете, какой из N цветных шариков вытащат из коробки. Чем более равномерно распределены шансы — тем больше «неожиданности» в каждом вытягивании.
* Математически:

  $$
    H(U) = \sum_{u} p(u)\,\log_2\frac1{p(u)}.
  $$
* Свойства:

  * **Ненегативность**: H(U) ≥ 0;
  * **Максимум при равновероятном распределении**: если все p(u)=1/|U|, то H(U)=\log\_2|U|;
  * **Аддитивность для независимых**: если X и Y независимы, H(X,Y)=H(X)+H(Y).&#x20;

---

### 2. Понятие Hₙ(U) и связь H(U), U\~p и Hₙ(U)

Здесь Hₙ(U) обычно означает энтропию при «неправильном» распределении q, то есть

$$
  H_q(U) = -\sum_u p(u)\,\log_2 q(u).
$$

* Это «перекрёстная энтропия», которая показывает, сколько бит понадобится, если кодировать с распределением q вместо истинного p.
* Связь:

  $$
    H_q(U) = H(U) + D_{\mathrm{KL}}(p\|q),
  $$

  где Dₖₗ(p‖q) – дивергенция Кульбака–Лейблера (см. вопрос 3).&#x20;

---

### 3. Условная энтропия и дивергенция Кульбака–Лейблера. Свойства условной энтропии

* **Условная энтропия H(X|Y)** – средняя неопределённость X при известном Y:

  $$
    H(X|Y)=\sum_{y}p(y)\,H(X|Y=y).
  $$
* **Дивергенция Dₖₗ(p‖q)** измеряет «расстояние» между распределениями p и q:

  $$
    D_{\mathrm{KL}}(p\|q)=\sum_x p(x)\log_2\frac{p(x)}{q(x)}.
  $$
* Свойства H(X|Y):

  * **Ненегативность**: H(X|Y) ≥ 0;
  * **H(X|Y) ≤ H(X)**: знание Y не увеличивает неопределённость про X;
  * **Аддитивность**: H(X,Y)=H(Y)+H(X|Y).&#x20;

---

### 4. Совместная энтропия. Энтропия системы независимых СВ. Свойства совместной энтропии

* **Совместная энтропия H(X,Y)** – мера «сюрприза» при паре (X,Y):

  $$
    H(X,Y)=-\sum_{x,y}p(x,y)\log_2 p(x,y).
  $$
* Если X и Y независимы, то p(x,y)=p(x)p(y) ⇒ H(X,Y)=H(X)+H(Y).
* Свойства: симметрия H(X,Y)=H(Y,X), и связь с условной: H(X,Y)=H(Y)+H(X|Y).&#x20;

---

### 5. Понятие взаимной информации

**I(X;Y)** показывает, сколько информации об X даёт наблюдение Y:

$$
  I(X;Y)=H(X)-H(X|Y)=H(Y)-H(Y|X).
$$

* Аналог: перекрёстное пересечение двух кругов «информации» X и Y.
* I(X;Y)≥0 и равно нулю, если X и Y независимы.&#x20;

---

### 6. Базовые свойства взаимной информации

* **Ненегативность**: I(X;Y) ≥ 0.
* **Симметрия**: I(X;Y)=I(Y;X).
* **Аддитивность для независимых пар**: если (X₁,Y₁) ⫫ (X₂,Y₂), то I((X₁,X₂);(Y₁,Y₂))=I(X₁;Y₁)+I(X₂;Y₂).&#x20;

---

### 7. Выпуклость дивергенции Кульбака–Лейблера

Функция (p,q)↦Dₖₗ(p‖q) выпукла по своей паре распределений:

$$
  D_{\mathrm{KL}}(\lambda p_1+(1-\lambda)p_2 \|\lambda q_1+(1-\lambda)q_2)
  \le \lambda D_{\mathrm{KL}}(p_1\|q_1)+(1-\lambda)D_{\mathrm{KL}}(p_2\|q_2).
$$

Интуиция: «смешанный» подход не даёт больше расхождения, чем смешение отдельных дивергенций.&#x20;

---

### 8. Закон больших чисел

Если X₁, …,Xₙ — iid с математическим ожиданием μ, то

$$
  \frac1n\sum_{i=1}^n X_i \xrightarrow{p} μ,
$$

то есть среднее по выборке сходится к истинному среднему при n→∞.&#x20;

---

### 9. AEP-теорема

**Асимптотическая равномерная вероятность (Asymptotic Equipartition Property)** говорит, что для большого n подавляющее число последовательностей длины n имеют вероятность примерно 2^{-nH}, где H — энтропия источника. Тем самым «типичное множество» содержит почти всю массу вероятности.&#x20;

---

### 10. Определение типичного множества. Основные свойства типичных множеств

* **Типичное множество Aₙ^ε** – все последовательности x^n, чья эмпирическая энтропия близка к H:

  $$
    \left|\!-\tfrac1n\log p(x^n)-H\right| < ε.
  $$
* Свойства:

  * **Высокая вероятность**: P(X^n∈Aₙ^ε) → 1;
  * **Мощность ≈ 2^{nH}**;
  * **Почти равные вероятности внутри Aₙ^ε**.&#x20;

---

### 11. Мощность типичного множества

|Aₙ^ε|≈2^{nH}. То есть количество «типичных» последовательностей растёт экспоненциально с n, с показателем H.&#x20;

---

### 12. Теорема о вероятности типичного множества

P(X^n∈Aₙ^ε) ≥1−δ для любых ε>0 и достаточно большого n. Это гарантирует, что почти все наблюдаемые последовательности будут «типичными».&#x20;

---

### 13. Теорема Шеннона о кодировании источника

Для дискретного источника U с энтропией H(U):

* **Прямая часть**: можно построить код длины ≈nH бит/символ с малой вероятностью ошибки декодирования.
* **Обратная часть**: нельзя сделать среднюю длину меньше H(U).&#x20;

---

### 14. Понятие высоковероятного множества. Связь типичного множества и высоковероятного множества

Высоковероятное множество – любой набор X^n с P(X^n) ≥1−ε.
Типичное множество – пример высоковероятного с дополнительно почти равными вероятностями внутри.&#x20;

---

### 15. Понятие префиксного и однозначно-декодируемого кода

* **Префиксный код**: ни один кодовыйword не является началом другого (пример: коды Хаффмана).
* **Однозначно-декодируемый**: любую последовательность кодов можно разбить лишь одним способом.
  Префиксность ⇒ однозначность.&#x20;

---

### 16. Кодирование источника с диадическим распределением

Диадическое распределение: p(u)=2^{-k} для целых k. Тогда можно строить простые двоичные коды, длина которых целочисленна.

---

### 17. Свойства диадического распределения

* Длины кода l(u)=−log₂p(u) целочисленны;
* Средняя длина равна энтропии: EL=H(U).

---

### 18. Коды Шеннона

Шеннон предложил код, где каждому символу u даётся длина ⌈−log₂p(u)⌉. Это гарантирует EL\<H(U)+1.

---

### 19. Неравенство Крафта

Для префиксных кодов длины l₁,…,l\_m должно выполняться

$$
  \sum_{i=1}^m 2^{-l_i} \le1.
$$

Обратное также верно: любое множество длин, удовлетворяющее этому неравенству, задаёт префиксный код.

---

### 20. Коды Хаффмана. Оптимальность кодов Хаффмана

Хаффман строит префиксный код, минимизирующий среднюю длину для заданных p(u). Он объединяет наименее вероятные символы рекурсивно. Оптимальность гарантируется жадным алгоритмом.

---

### 21. Понятие кодовой схемы. Достижимая скорость передачи кодовой схемы

Кодовая схема = выбор кодов для блоков символов + правила кодирования/декодирования.
Достижимая скорость R = (число информационных бит)/(число переданных символов).

---

### 22. Пропускная способность канала. Примеры

**C** – максимальная скорость передачи через канал при произвольно малой ошибке.

* Для двоичного симметричного канала (BSC с вероятностью ошибки τ):

  $$
    C = 1 - H_2(τ),
  $$

  где H₂ – бинарная энтропия.&#x20;

---

### 23. Пропускная способность двоичного симметричного канала

См. предыдущий пункт: C\_BSC(τ)=1−H₂(τ).&#x20;

---

### 24. Пропускная способность двоичного стирающего канала

Для BEC (вероятность «стирания» ε):

$$
  C = 1 - ε,
$$

потому что при стирании бит просто исчезает и требует повторной передачи.

---

### 25. Дифференциальная энтропия и взаимная информация. Их свойства

* **Дифференциальная энтропия h(X)** для непрерывной X с плотностью f(x):

  $$
    h(X) = -\int f(x)\log_2 f(x)\,dx.
  $$
* **Взаимная информация** I(X;Y)=h(X)−h(X|Y) сохраняет те же свойства (ненегативность, симметрию).

---

### 26. Энтропия нормального распределения

Для X∼N(μ,σ²):

$$
  h(X) = \tfrac12\log_2(2πeσ^2).
$$

Это максимальная дифференциальная энтропия при данном разбросе σ².

---

Надеюсь, такой обзор поможет Вам глубоко понять каждую тему. Если что-то осталось неясным или хочется примеры расчётов, дайте знать!

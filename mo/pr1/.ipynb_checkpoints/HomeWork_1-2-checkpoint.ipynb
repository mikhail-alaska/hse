{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7GHPoMtZcof"
   },
   "source": [
    "# Домашнее задание: Реализация kNN, наивного байесовского классификатора, LDA, QDA\n",
    "\n",
    "Цель: Реализовать с нуля 4 классификатора, обучить их на датасете, визуализировать результаты, сравнить качество.\n",
    "\n",
    "Запрещено использовать готовые реализации из sklearn:\n",
    "   - sklearn.neighbors.KNeighborsClassifier\n",
    "   - sklearn.naive_bayes.*\n",
    "   - sklearn.discriminant_analysis.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IbKp23wHZeFV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (105, 4), Test size: (45, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from scipy.spatial.distance import cdist\n",
    "import pandas as pd\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Train size: {X_train.shape}, Test size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sLxbqAXrZrPP"
   },
   "outputs": [],
   "source": [
    "class MyKNNClassifier:\n",
    "    def __init__(self, k=5, weighted=False, metric='euclidean'):\n",
    "        \"\"\"\n",
    "        Инициализация классификатора kNN.\n",
    "\n",
    "        Параметры:\n",
    "        - k: количество соседей\n",
    "        - weighted: использовать взвешенное голосование или нет\n",
    "        - metric: метрика расстояния. Поддерживаемые значения:\n",
    "                  'euclidean', 'manhattan', 'chebyshev'\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.weighted = weighted\n",
    "        self.metric = metric\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y)\n",
    "        if X.ndim != 2:\n",
    "            raise ValueError(\"X должен иметь форму (n_samples, n_features)\")\n",
    "        if y.ndim != 1:\n",
    "            raise ValueError(\"y должен иметь форму (n_samples,)\")\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"Число объектов в X и y должно совпадать\")\n",
    "        if self.k < 1:\n",
    "            raise ValueError(\"k должно быть >= 1\")\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        return self\n",
    "\n",
    "    def _compute_distances(self, X):\n",
    "        \"\"\"\n",
    "        Вычисляет расстояния от каждой точки в X до всех точек в X_train.\n",
    "\n",
    "        Возвращает матрицу расстояний формы (n_samples, n_train_samples)\n",
    "        \"\"\"\n",
    "        if self.X_train is None:\n",
    "            raise RuntimeError(\"Сначала вызовите fit()\")\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        if X.ndim != 2:\n",
    "            raise ValueError(\"X должен иметь форму (n_samples, n_features)\")\n",
    "        if X.shape[1] != self.X_train.shape[1]:\n",
    "            raise ValueError(\"Число признаков в X и X_train должно совпадать\")\n",
    "        if self.metric == 'euclidean':\n",
    "            diff = X[:, None, :] - self.X_train[None, :, :]    \n",
    "            distances = np.sqrt(np.sum(diff ** 2, axis=2))  \n",
    "\n",
    "        elif self.metric == 'manhattan':\n",
    "            diff = np.abs(X[:, None, :] - X_train[None, :, :])  \n",
    "            distances = np.sum(diff, axis=2)\n",
    "\n",
    "        elif self.metric == 'chebyshev':\n",
    "            diff = np.abs(X[:, None, :] - X_train[None, :, :])\n",
    "            distances = np.max(diff, axis=2)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Метрика '{self.metric}' не поддерживается. \"\n",
    "                \"Используйте: 'euclidean', 'manhattan', 'chebyshev'.\"\n",
    "            )\n",
    "        return distances\n",
    "\n",
    "    def predict(self, X):\n",
    "        # 1. Посчитайте расстояния от каждой точки в X до всех точек в X_train (_compute_distances)\n",
    "        # 2. Для каждой точки найдите индексы k ближайших соседей\n",
    "        # 3. Получите метки этих соседей\n",
    "        # 4. Если weighted=False — голосование по большинству\n",
    "        #    Если weighted=True — взвешенное голосование (вес = 1 / (расстояние + 1e-5))\n",
    "        # 5. Верните предсказанные метки\n",
    "        # 1) матрица расстояний\n",
    "        if self.X_train is None:\n",
    "            raise RuntimeError(\"Сначала вызовите fit()\")\n",
    "        distances = self._compute_distances(X)\n",
    "        n_test, n_train = distances.shape\n",
    "        k = min(self.k, n_train)\n",
    "\n",
    "        y_pred = np.empty(n_test, dtype=self.y_train.dtype)\n",
    "\n",
    "        eps = 1e-5  \n",
    "        for i in range(n_test):\n",
    "            d = distances[i]  # (n_train,)\n",
    "\n",
    "            # 2) индексы k ближайших соседей\n",
    "            nn_idx = np.argpartition(d, k-1)[:k]\n",
    "            nn_idx = nn_idx[np.argsort(d[nn_idx])]\n",
    "\n",
    "            # 3) метки и расстояния соседей\n",
    "            labels = self.y_train[nn_idx]\n",
    "            dists = d[nn_idx]\n",
    "\n",
    "            if self.weighted:\n",
    "                # 4) взвешенное голосование\n",
    "                weights = 1.0 / (dists + eps)\n",
    "                sums = {}\n",
    "                for lbl, w, dd in zip(labels, weights, dists):\n",
    "                    if lbl not in sums:\n",
    "                        sums[lbl] = {\"w\": 0.0, \"sumd\": 0.0, \"cnt\": 0}\n",
    "                    sums[lbl][\"w\"] += w\n",
    "                    sums[lbl][\"sumd\"] += dd\n",
    "                    sums[lbl][\"cnt\"] += 1\n",
    "                best_lbl, best_w, best_avgd = None, -np.inf, np.inf\n",
    "                for lbl, agg in sums.items():\n",
    "                    avgd = agg[\"sumd\"] / agg[\"cnt\"]\n",
    "                    if (agg[\"w\"] > best_w) or (np.isclose(agg[\"w\"], best_w) and avgd < best_avgd):\n",
    "                        best_lbl, best_w, best_avgd = lbl, agg[\"w\"], avgd\n",
    "                y_pred[i] = best_lbl\n",
    "            else:\n",
    "                # 4) простое большинство\n",
    "                counts = {}\n",
    "                sums = {}\n",
    "                for lbl, dd in zip(labels, dists):\n",
    "                    counts[lbl] = counts.get(lbl, 0) + 1\n",
    "                    sums[lbl] = sums.get(lbl, 0.0) + dd\n",
    "                best_lbl, best_cnt, best_avgd = None, -1, np.inf\n",
    "                for lbl in counts:\n",
    "                    avgd = sums[lbl] / counts[lbl]\n",
    "                    if (counts[lbl] > best_cnt) or (counts[lbl] == best_cnt and avgd < best_avgd):\n",
    "                        best_lbl, best_cnt, best_avgd = lbl, counts[lbl], avgd\n",
    "                y_pred[i] = best_lbl\n",
    "\n",
    "        # 5) вернуть предсказания\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyfCIKISdI1n"
   },
   "source": [
    "## Наивный байесовский классификатор (Gaussian Naive Bayes)\n",
    "\n",
    "Наивный байесовский классификатор основан на **теореме Байеса** и предположении о **условной независимости признаков** относительно класса.\n",
    "\n",
    "---\n",
    "\n",
    "### Теорема Байеса:\n",
    "\n",
    "Вероятность принадлежности объекта к классу $ c $ при наблюдении признакового вектора $ \\mathbf{x} = (x_1, x_2, \\dots, x_d) $:\n",
    "\n",
    "$$\n",
    "P(y = c \\mid \\mathbf{x}) = \\frac{P(\\mathbf{x} \\mid y = c) \\cdot P(y = c)}{P(\\mathbf{x})}\n",
    "$$\n",
    "\n",
    "Так как $ P(\\mathbf{x}) $ одинакова для всех классов, для классификации достаточно максимизировать **числитель** — **апостериорную вероятность**:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\underset{c}{\\mathrm{argmax}} \\; P(y = c) \\cdot P(\\mathbf{x} \\mid y = c)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Предположение \"наивности\":\n",
    "\n",
    "Признаки условно независимы при заданном классе:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{x} \\mid y = c) = \\prod_{j=1}^{d} P(x_j \\mid y = c)\n",
    "$$\n",
    "\n",
    "Тогда:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\underset{c}{\\mathrm{argmax}} \\; P(y = c) \\cdot \\prod_{j=1}^{d} P(x_j \\mid y = c)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Для непрерывных признаков: предположение о нормальном распределении\n",
    "\n",
    "Часто предполагают, что каждый признак $ x_j $ в классе $ c $ распределён нормально:\n",
    "\n",
    "$$\n",
    "x_j \\mid y = c \\; \\sim \\; \\mathcal{N}(\\mu_{jc}, \\sigma_{jc}^2)\n",
    "$$\n",
    "\n",
    "Тогда:\n",
    "\n",
    "$$\n",
    "P(x_j \\mid y = c) = \\frac{1}{\\sqrt{2\\pi \\sigma_{jc}^2}} \\exp\\left( -\\frac{(x_j - \\mu_{jc})^2}{2\\sigma_{jc}^2} \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Что нужно оценить на обучающей выборке:\n",
    "\n",
    "- Априорные вероятности:  \n",
    "  $ \\pi_c = \\frac{\\text{количество объектов класса } c}{\\text{общее количество объектов}} $\n",
    "\n",
    "- Средние:  \n",
    "  $ \\mu_{jc} = \\frac{1}{n_c} \\sum_{i: y_i = c} x_{ij} $\n",
    "\n",
    "- Дисперсии:  \n",
    "  $ \\sigma_{jc}^2 = \\frac{1}{n_c} \\sum_{i: y_i = c} (x_{ij} - \\mu_{jc})^2 $\n",
    "\n",
    "(где $ n_c $ — количество объектов класса $ c $)\n",
    "\n",
    "---\n",
    "\n",
    "**Ваша задача**: реализовать этот классификатор с нуля в классе `MyGaussianNB`, используя описанные формулы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VzDjyqf-ZyZ4"
   },
   "outputs": [],
   "source": [
    "class MyGaussianNB:\n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.mean = None       # среднее по каждому признаку для каждого класса\n",
    "        self.var = None        # дисперсия по каждому признаку для каждого класса\n",
    "        self.priors = None     # априорные вероятности классов\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # 1. Найдите уникальные классы\n",
    "        # 2. Для каждого класса:\n",
    "        #    - оцените mean и var для каждого признака\n",
    "        #    - оцените prior = количество примеров класса / всего примеров\n",
    "        # 3. Сохраните всё в атрибуты класса\n",
    "        print(x)\n",
    "        print(y)\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y)\n",
    "        if X.ndim != 2:\n",
    "            raise ValueError(\"X должен иметь форму (n_samples, n_features)\")\n",
    "        if y.ndim != 1 or y.shape[0] != X.shape[0]:\n",
    "            raise ValueError(\"y должен иметь форму (n_samples,) и совпадать по длине с X\")\n",
    "\n",
    "        self.classes, counts = np.unique(y, return_counts=True)\n",
    "        n_classes = self.classes.shape[0]\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        self.mean = np.zeros((n_classes, n_features), dtype=float)\n",
    "        self.var  = np.zeros((n_classes, n_features), dtype=float)\n",
    "        self.priors = counts / float(X.shape[0])\n",
    "\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            Xc = X[y == c]\n",
    "            self.mean[idx, :] = Xc.mean(axis=0)\n",
    "            self.var[idx, :] = Xc.var(axis=0)\n",
    "\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        # 1. Для каждой точки X и каждого класса вычислите логарифм правдоподобия + логарифм prior\n",
    "        #    (используйте формулу нормального распределения в логарифмическом виде)\n",
    "        # 2. Выберите класс с максимальным значением\n",
    "        # 3. Верните предсказания\n",
    "        if self.mean is None:\n",
    "            raise RuntimeError(\"Сначала вызовите fit()\")\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        if X.ndim != 2 or X.shape[1] != self.mean.shape[1]:\n",
    "            raise ValueError(\"X должен иметь форму (n_samples, n_features) c тем же числом признаков, что и при fit()\")\n",
    "\n",
    "        eps = 1e-9\n",
    "        var = self.var + eps\n",
    "        log_priors = np.log(self.priors)\n",
    "\n",
    "        diff = X[:, None, :] - self.mean[None, :, :]                 \n",
    "        log_likelihood = -0.5 * (np.log(2.0 * np.pi * var)[None, :, :] + (diff ** 2) / var[None, :, :])\n",
    "        log_likelihood = log_likelihood.sum(axis=2)                  \n",
    "\n",
    "        log_posterior = log_likelihood + log_priors[None, :]         \n",
    "        class_indices = np.argmax(log_posterior, axis=1)             \n",
    "        return self.classes[class_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBXHHPTJeCk_"
   },
   "source": [
    "## Линейный (LDA) и квадратичный (QDA) дискриминантный анализ Фишера\n",
    "\n",
    "LDA и QDA — это **параметрические методы классификации**, основанные на предположении, что признаки в каждом классе распределены **нормально**.\n",
    "\n",
    "---\n",
    "\n",
    "### Общая постановка\n",
    "\n",
    "Пусть дано:\n",
    "- $ \\mathbf{x} \\in \\mathbb{R}^d $ — вектор признаков,\n",
    "- $ y \\in \\{1, 2, \\dots, K\\} $ — метка класса,\n",
    "- $ \\pi_k = P(y = k) $ — априорная вероятность класса $ k $,\n",
    "- $ f_k(\\mathbf{x}) = P(\\mathbf{x} \\mid y = k) $ — функция правдоподобия.\n",
    "\n",
    "Предполагаем, что:\n",
    "$$\n",
    "\\mathbf{x} \\mid y = k \\; \\sim \\; \\mathcal{N}(\\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k)\n",
    "$$\n",
    "\n",
    "Тогда:\n",
    "$$\n",
    "f_k(\\mathbf{x}) = \\frac{1}{(2\\pi)^{d/2} |\\mathbf{\\Sigma}_k|^{1/2}} \\exp\\left( -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_k)^\\top \\mathbf{\\Sigma}_k^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k) \\right)\n",
    "$$\n",
    "\n",
    "По теореме Байеса:\n",
    "$$\n",
    "P(y = k \\mid \\mathbf{x}) = \\frac{\\pi_k f_k(\\mathbf{x})}{\\sum_{l=1}^K \\pi_l f_l(\\mathbf{x})}\n",
    "$$\n",
    "\n",
    "Классификатор выбирает класс с **максимальной апостериорной вероятностью**:\n",
    "$$\n",
    "\\hat{y} = \\underset{k}{\\mathrm{argmax}} \\; \\pi_k f_k(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Переход к дискриминантным функциям\n",
    "\n",
    "Вместо сравнения вероятностей удобнее сравнивать **логарифмы** (монотонное преобразование):\n",
    "\n",
    "$$\n",
    "\\delta_k(\\mathbf{x}) = \\log \\left( \\pi_k f_k(\\mathbf{x}) \\right) = \\log \\pi_k + \\log f_k(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "Подставим выражение для $ f_k(\\mathbf{x}) $:\n",
    "\n",
    "$$\n",
    "\\delta_k(\\mathbf{x}) = \\log \\pi_k - \\frac{1}{2} \\log |\\mathbf{\\Sigma}_k| - \\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_k)^\\top \\mathbf{\\Sigma}_k^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k) + \\text{const}\n",
    "$$\n",
    "\n",
    "(где `const` не зависит от $ k $, поэтому его можно игнорировать при сравнении)\n",
    "\n",
    "---\n",
    "\n",
    "## Линейный дискриминантный анализ (LDA)\n",
    "\n",
    "**Предположение**: ковариационные матрицы **одинаковы** для всех классов:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Sigma}_k = \\mathbf{\\Sigma} \\quad \\forall k\n",
    "$$\n",
    "\n",
    "Тогда:\n",
    "- $ \\log |\\mathbf{\\Sigma}_k| $ — константа → исключается,\n",
    "- $ \\mathbf{\\Sigma}_k^{-1} = \\mathbf{\\Sigma}^{-1} $ — общая для всех классов.\n",
    "\n",
    "Раскрываем квадратичную форму:\n",
    "\n",
    "$$\n",
    "(\\mathbf{x} - \\boldsymbol{\\mu}_k)^\\top \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k) = \\mathbf{x}^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{x} - 2 \\boldsymbol{\\mu}_k^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{x} + \\boldsymbol{\\mu}_k^\\top \\mathbf{\\Sigma}^{-1} \\boldsymbol{\\mu}_k\n",
    "$$\n",
    "\n",
    "Первое слагаемое не зависит от $ k $ → исключается.\n",
    "\n",
    "Остаётся **линейная** дискриминантная функция:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\delta_k^{\\text{LDA}}(\\mathbf{x}) = \\boldsymbol{\\mu}_k^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{x} - \\frac{1}{2} \\boldsymbol{\\mu}_k^\\top \\mathbf{\\Sigma}^{-1} \\boldsymbol{\\mu}_k + \\log \\pi_k\n",
    "}\n",
    "$$\n",
    "\n",
    "Решающие границы — **линейные** (гиперплоскости).\n",
    "\n",
    "---\n",
    "\n",
    "## Квадратичный дискриминантный анализ (QDA)\n",
    "\n",
    "**Предположение**: ковариационные матрицы **разные** для каждого класса: $ \\mathbf{\\Sigma}_k $.\n",
    "\n",
    "Тогда дискриминантная функция остаётся **квадратичной**:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\delta_k^{\\text{QDA}}(\\mathbf{x}) = -\\frac{1}{2} \\log |\\mathbf{\\Sigma}_k| - \\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_k)^\\top \\mathbf{\\Sigma}_k^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k) + \\log \\pi_k\n",
    "}\n",
    "$$\n",
    "\n",
    "Решающие границы — **квадратичные поверхности** (параболы, эллипсы и т.д. в 2D).\n",
    "\n",
    "---\n",
    "\n",
    "## Что нужно оценить на обучающей выборке\n",
    "\n",
    "Для обоих методов:\n",
    "\n",
    "- **Априорные вероятности**:  \n",
    "  $ \\pi_k = \\frac{n_k}{n} $, где $ n_k $ — число объектов класса $ k $\n",
    "\n",
    "- **Средние векторы**:  \n",
    "  $ \\boldsymbol{\\mu}_k = \\frac{1}{n_k} \\sum_{i: y_i = k} \\mathbf{x}_i $\n",
    "\n",
    "Для **LDA**:\n",
    "- **Общая ковариационная матрица**:\n",
    "  $$\n",
    "  \\mathbf{\\Sigma} = \\frac{1}{n - K} \\sum_{k=1}^K \\sum_{i: y_i = k} (\\mathbf{x}_i - \\boldsymbol{\\mu}_k)(\\mathbf{x}_i - \\boldsymbol{\\mu}_k)^\\top\n",
    "  $$\n",
    "\n",
    "Для **QDA**:\n",
    "- **Ковариационная матрица для каждого класса**:\n",
    "  $$\n",
    "  \\mathbf{\\Sigma}_k = \\frac{1}{n_k - 1} \\sum_{i: y_i = k} (\\mathbf{x}_i - \\boldsymbol{\\mu}_k)(\\mathbf{x}_i - \\boldsymbol{\\mu}_k)^\\top\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Ваша задача**: реализовать оба метода в классе `MyLDAQDA` с параметром `mode='LDA'` или `'QDA'`, используя приведённые формулы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AM-OZhqqcjbv"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyLDAQDA:\n",
    "    def __init__(self, mode='LDA'):\n",
    "        if mode not in ['LDA', 'QDA']:\n",
    "            raise ValueError(\"mode должен быть 'LDA' или 'QDA'\")\n",
    "        self.mode = mode\n",
    "        self.classes = None\n",
    "        self.mean = None\n",
    "        self.cov = None        \n",
    "        self.priors = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y)\n",
    "        if X.ndim != 2 or y.ndim != 1 or X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"Неверные размеры X или y\")\n",
    "\n",
    "        self.classes, counts = np.unique(y, return_counts=True)\n",
    "        n_classes = len(self.classes)\n",
    "        n_features = X.shape[1]\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        self.mean = np.zeros((n_classes, n_features))\n",
    "        self.priors = counts / n_samples\n",
    "\n",
    "        if self.mode == 'QDA':\n",
    "            self.cov = []\n",
    "\n",
    "        if self.mode == 'LDA':\n",
    "            Sw = np.zeros((n_features, n_features))\n",
    "\n",
    "        for idx, cls in enumerate(self.classes):\n",
    "            Xc = X[y == cls]\n",
    "            μ = Xc.mean(axis=0)\n",
    "            self.mean[idx] = μ\n",
    "            cov_c = np.cov(Xc.T, bias=True) + np.eye(n_features) * 1e-6\n",
    "\n",
    "            if self.mode == 'QDA':\n",
    "                self.cov.append(cov_c)\n",
    "            elif self.mode == 'LDA':\n",
    "                Sw += cov_c * (Xc.shape[0] / n_samples)\n",
    "\n",
    "        if self.mode == 'LDA':\n",
    "            self.cov = Sw + np.eye(n_features) * 1e-6 \n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        if X.ndim != 2 or self.mean is None:\n",
    "            raise RuntimeError(\"Вызовите fit() перед predict()\")\n",
    "\n",
    "        n_classes = len(self.classes)\n",
    "        n_samples = X.shape[0]\n",
    "        scores = np.zeros((n_samples, n_classes))\n",
    "\n",
    "        for idx, cls in enumerate(self.classes):\n",
    "            μ = self.mean[idx]\n",
    "            prior = self.priors[idx]\n",
    "\n",
    "            if self.mode == 'LDA':\n",
    "                Σ_inv = np.linalg.inv(self.cov)\n",
    "                xΣ⁻¹μ = X @ Σ_inv @ μ\n",
    "                μΣ⁻¹μ = μ.T @ Σ_inv @ μ\n",
    "                delta = xΣ⁻¹μ - 0.5 * μΣ⁻¹μ + np.log(prior)\n",
    "\n",
    "            elif self.mode == 'QDA':\n",
    "                Σ = self.cov[idx]\n",
    "                Σ_inv = np.linalg.inv(Σ)\n",
    "                logdet = np.linalg.slogdet(Σ)[1]\n",
    "                diff = X - μ\n",
    "                mahal = np.sum(diff @ Σ_inv * diff, axis=1)\n",
    "                delta = -0.5 * logdet - 0.5 * mahal + np.log(prior)\n",
    "\n",
    "            scores[:, idx] = delta\n",
    "\n",
    "        pred_indices = np.argmax(scores, axis=1)\n",
    "        return self.classes[pred_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Tinhn26hbV-"
   },
   "source": [
    "## Для метода kNN:\n",
    "Проведите кросс-валидацию (например, 5-fold) на обучающей выборке для значений k = 1, 2, ..., 30.\n",
    "\n",
    "\n",
    "Для каждого k посчитайте среднюю Accuracy (долю верных ответов - среднюю по всем классам) по фолдам.\n",
    "\n",
    "\n",
    "Постройте график: k → accuracy (два графика: обычный и взвешенный kNN).\n",
    "\n",
    "Выберите k, при котором accuracy максимальна — используйте его для финального тестирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ggQDOx_diOW9"
   },
   "outputs": [],
   "source": [
    "# Ваш код ниже"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVApo6TxhxDO"
   },
   "source": [
    "## Для наивного байесовского классификатора и линейного и квадратичного дискриминантного анализа\n",
    "\n",
    "Обучите модели на обучающей выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Fqrj-JFKfrpq"
   },
   "outputs": [],
   "source": [
    "# Ваш код ниже"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBVu_sLJji1W"
   },
   "source": [
    "## Для всех моделей\n",
    "\n",
    "Вызовите методы для предсказания на данных из тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "kk--hddmj5nY"
   },
   "outputs": [],
   "source": [
    "# Ваш код ниже"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHyPFW7OiXhR"
   },
   "source": [
    "## Метрики качества классификации\n",
    "\n",
    "Для оценки качества классификаторов используются следующие метрики. Обозначения:\n",
    "\n",
    "- $ TP $ — True Positive (верно предсказанные положительные примеры)\n",
    "- $ TN $ — True Negative\n",
    "- $ FP $ — False Positive (ложные срабатывания)\n",
    "- $ FN $ — False Negative (пропущенные положительные)\n",
    "\n",
    "Для многоклассовой классификации используется **макро-усреднение** — метрика считается отдельно для каждого класса, затем берётся среднее.\n",
    "\n",
    "---\n",
    "\n",
    "### Accuracy (доля правильных ответов)\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Precision (точность — доля верных срабатываний среди всех предсказанных положительных)\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Recall (полнота — доля найденных положительных среди всех реальных положительных)\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### F1-мера (гармоническое среднее precision и recall)\n",
    "\n",
    "$$\n",
    "F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "F1-мера полезна, когда классы несбалансированы — она \"наказывает\" за сильный дисбаланс между precision и recall.\n",
    "\n",
    "### Задача:\n",
    "Для каждой модели подсчитать вышеописанные метрики для каждого из классов и вывести в виде таблицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "FFOe9lpWiWHZ"
   },
   "outputs": [],
   "source": [
    "# Ваш код ниже"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ii-3YvISk3rr"
   },
   "source": [
    "## Вопросы для подготовки к защите\n",
    "\n",
    "Что такое kNN? Почему он называется \"ленивым\" алгоритмом?\n",
    "\n",
    "Как работает взвешенный kNN? Какие функции весов вы знаете?\n",
    "\n",
    "Почему в kNN важно нормировать признаки?\n",
    "\n",
    "Как вы подбирали оптимальное k? Почему не стоит брать k=1 или k=N?\n",
    "\n",
    "В чём смысл кросс-валидации? Почему нельзя просто выбрать k по максимальной точности на обучающей выборке?\n",
    "\n",
    "Какие метрики расстояния вы реализовали? Чем отличается Манхэттенское расстояние от Евклидова? Когда какое лучше?\n",
    "\n",
    "Объясните формулу наивного байесовского классификатора. Почему он \"наивный\"?\n",
    "\n",
    "Какие предположения делает Gaussian Naive Bayes?\n",
    "\n",
    "Чем отличается LDA от QDA? В каких случаях какой метод предпочтительнее?\n",
    "\n",
    "Почему в LDA решающие границы — линейные, а в QDA — квадратичные? Покажите на формулах.\n",
    "\n",
    "Запишите формулы для accuracy, precision, recall и F1-score через TP, TN, FP, FN.\n",
    "\n",
    "Почему accuracy не всегда адекватная метрика? Приведите пример, когда accuracy высокая, но модель бесполезна.\n",
    "\n",
    "Что такое макро-усреднение (macro-average)? Почему мы его используем в многоклассовой классификации?\n",
    "\n",
    "В чём разница между macro-F1 и micro-F1? Когда какой предпочтительнее?\n",
    "\n",
    "Почему F1-мера — это гармоническое среднее, а не арифметическое? Как это влияет на интерпретацию?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNjfmeOVk5mM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
